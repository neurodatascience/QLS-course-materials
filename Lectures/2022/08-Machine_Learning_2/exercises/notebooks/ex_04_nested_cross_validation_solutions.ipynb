{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccd510b",
   "metadata": {},
   "source": [
    "# Performing nested cross-validation\n",
    "\n",
    "Here we will evaluate the performance of an L2-regularized logistic\n",
    "regression on one of the classification datasets distributed with\n",
    "scikit-learn.\n",
    "\n",
    "The model has a hyperparameter C, which controls the regularization strength:\n",
    "a higher value of C means less regularization. We will automatically select\n",
    "the appropriate value for C among a grid of possible values with a nested\n",
    "cross-validation loop.\n",
    "\n",
    "The whole procedure therefore looks like:\n",
    "\n",
    "- Outer loop:\n",
    "  - obtain 5 (train, test) splits for the whole dataset\n",
    "  - initialize `all_scores` to an empty list\n",
    "  - for each (train, test) split:\n",
    "    + run grid-search (ie inner CV loop) on training data and obtain a model\n",
    "      fitted to the whole training data with the best hyperparameter.\n",
    "    + evaluate the model on the test data\n",
    "    + append the resulting score to `all_scores`\n",
    "  - return `all_scores`\n",
    "\n",
    "- Grid-search (inner loop):\n",
    "  - obtain 3 (train, test) splits for the available data (the training data\n",
    "    from the outer loop)\n",
    "  - initialize `all_scores` to an empty list\n",
    "  - for each possible hyperparameter value C:\n",
    "    + initialize `scores_for_this_C` to an empty list\n",
    "    + for each  train, test split:\n",
    "      * fit a model on train, using the hyperparameter C\n",
    "      * evaluate the model on test\n",
    "      * append the resulting score to `scores_for_this_C`\n",
    "    + append the mean of `scores_for_this_C` to `all_scores`\n",
    "  - select the hyperparameter with the best mean score\n",
    "  - refit the model on the whole available data, using the selected\n",
    "    hyperparameter\n",
    "  - return this model\n",
    "\n",
    "Most of this logic is implemented in this module, but some key parts are\n",
    "still missing (marked with \"TODO\"). Your job is to complete the functions\n",
    "`cross_validate` and `grid_search` so that the whole nested cross-validation\n",
    "can be run.\n",
    "\n",
    "Some helper routines, `get_kfold_splits` and `fit_and_score`, are\n",
    "provided to make the task easier. Make sure you read their code and\n",
    "understand what they do.\n",
    "\n",
    "The docstrings of the incomplete functions document precisely what are their\n",
    "parameters, and what they should compute and return. Rely on this information\n",
    "to write your implementation.\n",
    "\n",
    "This nested cross-validation procedure is often used, so scikit-learn\n",
    "provides all the functionality we are implementing here. To check that our\n",
    "implementation is correct, we can therefore compare results with what we\n",
    "obtain from scikit-learn. At the end of the file, you will see code that\n",
    "loads a scikit-learn dataset, and computes cross-validation scores using our\n",
    "`cross_validate` function, then using scikit-learn, and prints both results.\n",
    "If you execute this script by running `python nested_cross_validation.py`,\n",
    "these two results will be shown and you can check that the code runs and\n",
    "produces correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f8f3b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model, model_selection, metrics\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1db6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Utilities\n",
    "\n",
    "The functions below are helpers for the main routines `cross_validate` and\n",
    "`grid_search`. You should read them but they do not need to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe78eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load iris data\n",
    "\n",
    "    This function shuffles the data (without breaking the pairing of X and y)\n",
    "    so that the examples are not sorted by class.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of length 2\n",
    "      X, design matrix of shape (n_samples, n_features)\n",
    "      y, targets, of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    X, y = datasets.load_iris(return_X_y=True)\n",
    "    idx = np.arange(len(y))\n",
    "    np.random.RandomState(0).shuffle(idx)\n",
    "    # this is now the recommended way of doing this, but only works with recent\n",
    "    # versions of numpy:\n",
    "    # np.random.default_rng(0).shuffle(idx)\n",
    "    X, y = X[idx], y[idx]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_splits(n_samples, k):\n",
    "    \"\"\"Given a total number of samples, return k-fold (train, test) indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "      The total number of samples in the dataset to be split for\n",
    "      cross-validation.\n",
    "\n",
    "    k : int, optional\n",
    "      The number of cross-validation folds\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    splits : list[tuple[np.array[int], np.array[int]]]\n",
    "      each element of `splits` corresponds to one cross-validation fold and\n",
    "      contains a pair of arrays (train, test):\n",
    "      - train: the integer indices of samples in the training set\n",
    "      - test: the integer indices of samples in the testing set\n",
    "\n",
    "    \"\"\"\n",
    "    indices = np.arange(n_samples)\n",
    "    test_mask = np.empty(n_samples, dtype=bool)\n",
    "    splits = []\n",
    "    start = 0\n",
    "    for i in range(k):\n",
    "        n_test = n_samples // k\n",
    "        if i < n_samples % k:\n",
    "            n_test += 1\n",
    "        stop = start + n_test\n",
    "        test_mask[:] = False\n",
    "        test_mask[start:stop] = True\n",
    "        splits.append((indices[np.logical_not(test_mask)], indices[test_mask]))\n",
    "        start = stop\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91888faf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def fit_and_score(model, C, X, y, train_idx, test_idx, score_fun):\n",
    "    \"\"\"Fit a model on training data and compute its score on test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn estimator (will not be modified)\n",
    "      the estimator to be evaluated\n",
    "\n",
    "    C : float\n",
    "     The value for the regularization hyperparameter C to use when fitting the\n",
    "     model.\n",
    "\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "      the full design matrix\n",
    "\n",
    "    y : numpy array of shape (n_samples, n_outputs) or (n_samples,)\n",
    "      the full target vector\n",
    "\n",
    "    train_idx : sequence of ints\n",
    "      the indices of training samples (row indices of X)\n",
    "\n",
    "    test_idx : sequence of ints\n",
    "      the indices of testing samples\n",
    "\n",
    "    score_fun : callable\n",
    "      the function that measures performance on test data, with signature\n",
    "     `score = score_fun(true_y, predicted_y)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      The prediction score on test data\n",
    "\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    model.set_params(C=C)\n",
    "    model.fit(X[train_idx], y[train_idx])\n",
    "    predictions = model.predict(X[test_idx])\n",
    "    score = score_fun(y[test_idx], predictions)\n",
    "    print(f\"    Inner CV loop: fit and evaluate one model; score = {score:.2f}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1caf63d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Exercises\n",
    "\n",
    "The two functions below are incomplete! Complete the body of each function so\n",
    "that it behaves as described in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3891970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, C_candidates, X, y, inner_k, score_fun):\n",
    "    \"\"\"Inner loop of a nested cross-validation\n",
    "\n",
    "    This function estimates the performance of each hyperparameter in\n",
    "    `C_candidates` with cross validation. It then selects the best\n",
    "    hyperparameter and refits a model on the whole data using the selected\n",
    "    hyperparameter. The fitted model is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn estimator\n",
    "      The base estimator, copies of which are trained and evaluated. `model`\n",
    "      itself is not modified.\n",
    "\n",
    "    C_candidates : list[float]\n",
    "      list of possible values for the hyperparameter C.\n",
    "\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "      the design matrix\n",
    "\n",
    "    y : numpy array of shape (n_samples, n_outputs) or (n_samples,)\n",
    "      the target vector\n",
    "\n",
    "    inner_k : int\n",
    "      number of cross-validation folds\n",
    "\n",
    "    score_fun : callable\n",
    "      the function computing the score on test data, with signature\n",
    "      `score = score_fun(true_y, predicted_y)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_model : scikit-learn estimator\n",
    "      A copy of `model`, fitted on the whole `(X, y)` data, with the\n",
    "      (estimated) best hyperparameter.\n",
    "\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "    for C in C_candidates:\n",
    "        print(f\"  Grid search: evaluate hyperparameter C = {C}\")\n",
    "        # **TODO** : run 3-fold cross-validation loop, using this particular\n",
    "        # hyperparameter C. Compute the mean of scores across cross-validation\n",
    "        # folds and append it to `all_scores`.\n",
    "        # TODO_BEGIN\n",
    "        scores_for_this_C = []\n",
    "        for train_idx, test_idx in get_kfold_splits(len(y), inner_k):\n",
    "            score = fit_and_score(model, C, X, y, train_idx, test_idx, score_fun)\n",
    "            scores_for_this_C.append(score)\n",
    "        all_scores.append(np.mean(scores_for_this_C))\n",
    "        # TODO_END\n",
    "\n",
    "    # **TODO**: select the best hyperparameter according to the CV scores,\n",
    "    # refit the model on the whole data using this hyperparameter, and return\n",
    "    # the fitted model. Use `model.set_params` to set the hyperparameter\n",
    "    best_C = \"???\"\n",
    "    # TODO_BEGIN\n",
    "    best_C = C_candidates[np.argmax(all_scores)]\n",
    "    # TODO_END\n",
    "    print(f\"  ** Grid search: keep best hyperparameter C = {best_C} **\")\n",
    "    # `clone` is to work with a copy of `model` instead of modifying the\n",
    "    # argument itself.\n",
    "    best_model = clone(model)\n",
    "    # TODO ...\n",
    "    # TODO_BEGIN\n",
    "    best_model.set_params(C=best_C)\n",
    "    best_model.fit(X, y)\n",
    "    # TODO_END\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, C_candidates, X, y, k, inner_k, score_fun):\n",
    "    \"\"\"Get CV score with an inner CV loop to select hyperparameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn estimator, for example `LogisticRegression()`\n",
    "      The base model to fit and evaluate. `model` itself is not modified.\n",
    "\n",
    "    C_candidates : list[float]\n",
    "      list of possible values for the hyperparameter C.\n",
    "\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "      the design matrix\n",
    "\n",
    "    y : numpy array of shape (n_samples, n_outputs) or (n_samples,)\n",
    "      the target vector\n",
    "\n",
    "    k : int\n",
    "      the number of splits for the k-fold cross-validation.\n",
    "\n",
    "    inner_k : int\n",
    "      the number of splits for the nested cross-validation (hyperparameter\n",
    "      selection).\n",
    "\n",
    "    score_fun : callable\n",
    "      the function computing the score on test data, with signature\n",
    "      `score = score_fun(true_y, predicted_y)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : list[float]\n",
    "       The scores obtained for each of the cross-validation folds\n",
    "\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "    for i, (train_idx, test_idx) in enumerate(get_kfold_splits(len(y), k)):\n",
    "        print(f\"\\nOuter CV loop: fold {i}\")\n",
    "        best_model = grid_search(\n",
    "            model, C_candidates, X[train_idx], y[train_idx], inner_k, score_fun\n",
    "        )\n",
    "        predictions = best_model.predict(X[test_idx])\n",
    "        score = score_fun(y[test_idx], predictions)\n",
    "        print(f\"Outer CV loop: finished fold {i}, score: {score:.2f}\")\n",
    "        all_scores.append(score)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8f0ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def cross_validate_sklearn(model, C_candidates, X, y, k, inner_k, scoring):\n",
    "    \"\"\"CV and hyperparameter selection using scikit-learn.\n",
    "\n",
    "    This is used as a reference for the output our implementation should\n",
    "    produce.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn estimator, for example `LogisticRegression()`\n",
    "      The base model to fit and evaluate. `model` itself is not modified.\n",
    "\n",
    "    C_candidates : list[float]\n",
    "      list of possible values for the hyperparameter C.\n",
    "\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "      the design matrix\n",
    "\n",
    "    y : numpy array of shape (n_samples, n_outputs) or (n_samples,)\n",
    "      the target vector\n",
    "\n",
    "    k : int\n",
    "      the number of splits for the k-fold cross-validation.\n",
    "\n",
    "    inner_k : int\n",
    "      the number of splits for the nested cross-validation (hyperparameter\n",
    "      selection).\n",
    "\n",
    "    scoring : str\n",
    "      name of a scikit-learn metric\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : list[float]\n",
    "       The scores obtained for each of the cross-validation folds\n",
    "    \"\"\"\n",
    "    grid_search_model = model_selection.GridSearchCV(\n",
    "        model,\n",
    "        {\"C\": C_candidates},\n",
    "        cv=model_selection.KFold(inner_k),\n",
    "        scoring=scoring,\n",
    "    )\n",
    "    return model_selection.cross_validate(\n",
    "        grid_search_model, X, y, cv=model_selection.KFold(k), scoring=scoring\n",
    "    )[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e24049",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Trying our routines on real data\n",
    "\n",
    "The code below gets executed when you run this script with `python\n",
    "nested_cross_validation`. It computes a cross-validation score with our code,\n",
    "and compares it to the results obtained with scikit-learn. Read it to see how\n",
    "what we have implemented would be easily done with scikit-learn.\n",
    "\n",
    "Here we have written this logic ourselves to understand how it works, but in\n",
    "practice in real projects we would use the scikit-learn functionality which\n",
    "is more flexible, more reliable and faster.\n",
    "\n",
    "Note: this code will only run once you have completed the exercises!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7405fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data()\n",
    "model = linear_model.LogisticRegression()\n",
    "C_candidates = [0.0001, 0.001, 0.01, 0.1]\n",
    "k, inner_k = 5, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14417ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(model, C_candidates, X, y, k, inner_k, metrics.accuracy_score)\n",
    "print(\"\\n\\nMy scores:\")\n",
    "print(scores)\n",
    "sklearn_scores = cross_validate_sklearn(model, C_candidates, X, y, k, inner_k, \"accuracy\")\n",
    "print(\"Scikit-learn scores:\")\n",
    "print(list(sklearn_scores))\n",
    "assert np.allclose(scores, sklearn_scores), \"Results differ from scikit-learn!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf9db1",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- When running the cross-validation procedure we have just implemented, how\n",
    "  many models did we fit in total?\n",
    "  - Answer: 5 * (3 * 4 + 1) = 65\n",
    "- There are 150 samples in the iris dataset. For this dataset, what is the\n",
    "  size of the 5 test sets in the outer loop? of each of the 3 validation sets\n",
    "  in the grid-search (inner loop)?\n",
    "  - Answer: outer loop: 150 / 5 = 30; inner loop: (150 * 4 / 5) / 3 = 40\n",
    "\n",
    "## Additional exercise (optional)\n",
    "\n",
    "Have you noticed the hyperparameter grid was specified slightly differently\n",
    "for the scikit-learn `GridSearchCV`? we passed a dictionary:\n",
    "`{\"C\": [0.0001, 0.001, 0.01, 0.1 ]}`.\n",
    "\n",
    "This is because with `GridSearchCV` we can specify values for several\n",
    "hyperparameters, for example:\n",
    "`{\"C\": [0.0001, 0.001, 0.01, 0.1], \"penalty\": [\"l1\", \"l2\"]}`,\n",
    "and all combinations of these will be tried.\n",
    "\n",
    "Modify this module so that we can specify such a hyperparameter grid, rather\n",
    "than only a list of values for a specific hyperparameter named \"C\". Hint:\n",
    "check the documentation for the `set_params` function of scikit-learn\n",
    "estimators. You may want to use the python dict unpacking syntax, for example\n",
    "`model.set_params(**hyperparams)`. You can also use `itertools.product` from\n",
    "the python standard library to easily build all the combinations of\n",
    "hyperparameters."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
