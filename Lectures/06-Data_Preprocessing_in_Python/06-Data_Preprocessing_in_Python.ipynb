{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dApheqXqV-Tf"
   },
   "source": [
    "# Data preprocessing: more art than science?\n",
    "### Written by Nadia Blostein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-R_4TaoV-Th"
   },
   "source": [
    "## Contents of this notebook:\n",
    "<ol>\n",
    "<li>Load and examine your data</li>\n",
    "<li>Data reformatting</li>\n",
    "<li>Data filtering</li>\n",
    "<li>Data transforms</li>\n",
    "<li>Data visualization</li>\n",
    "<li>Examining and manipulating 2D images with scikit image and scipy</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JrpQkXhYLKm"
   },
   "source": [
    "# Setup\n",
    "Fetch the dataset that you'll be working with throughout this assignment. Sometimes, open-access datasets are stored as a Github repository, in which case it is very straightforward to clone the repository of interest (refer to the [Git / Github module](https://github.com/neurodatascience/course-materials-2022/tree/main/Lectures/05-Git_GitHub) of QLSC612 if you are feeling rusty).\\\n",
    "**Note:** When invoking Bash (to interact with your command-line) from your Jupyter notebook, you often need to precede your command with `!` (or `%` in the case of the `cd` command)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PiZxGN8YY7t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NadiaBlostein/Open-Access-HCP-Data.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1L4WX4C_Synu"
   },
   "source": [
    "Examine your directory structure with the `ls` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1foEjAGSguZ"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVQjjmXWSHvo"
   },
   "source": [
    "Change the working directory of the notebook to be the `Open-Access-HCP-Data` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW23_nXkZRKn"
   },
   "outputs": [],
   "source": [
    "%cd Open-Access-HCP-Data \n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYL3sxnFSyUV"
   },
   "source": [
    "In this assignment, we will be working with data from the Human Connectome Project (HCP). You can read more about the data [here](https://github.com/NadiaBlostein/Open-Access-HCP-Data/blob/main/README.md). Specifically, you will preprocess `.csv` (in the `HCP_csv_data` folder) and `.png` files (in the `HCP_2D_slices_MRI_data` folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KmjJboZTW1Z"
   },
   "outputs": [],
   "source": [
    "!ls HCP_2D_slices_MRI_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMJ-aUYRTZvo"
   },
   "outputs": [],
   "source": [
    "!ls HCP_csv_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG2oM0BjV-Ti"
   },
   "source": [
    "# 1. Load and examine your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkzkrh-oV-Tj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"HCP_csv_data/unrestricted_HCP_behavioral.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVkqozdRV-Tk"
   },
   "source": [
    "**Question 1:** How many subjects (rows) and features (columns) do you have?\\\n",
    "_PS: You cannot always assume that your subjects are rows and features are columns, and sometimes there may be strange header rows with useless information that you need to remove._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eXqY0np7V-Tk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z5Y3exuzhd-"
   },
   "source": [
    "Feel free to examine your data frame in the empty code cell below. Examples of valuable commands for a preliminary look at one's data frame: `df.info()`, `df.describe()`, `df.columns()`, `df.head()`, `df.tail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR_zFqfAz7j7"
   },
   "source": [
    "Next, we will select the `Gender` column to count how many instances we have of each value.\\\n",
    "Note that the code below is using an [f-string](https://realpython.com/python-f-strings/) to make the format of what you are printing prettier (ie cleaner and clearer!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnKocnwxV-Tn"
   },
   "outputs": [],
   "source": [
    "print(f\"unique values of column: {df['Gender'].unique()}\") # prints list of unique values in a feature / column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nvalue counts of column:\\n{df['Gender'].value_counts()}\") # counts how many times each unique value in a feature / column occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Display only the rows associated with female (`F`) subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d4xPY2Wi5j0"
   },
   "source": [
    "**Question 3:** Is there another way to count how many males (`M`) and females (`F`) are in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "W3W5g75VV-Tn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgl2Bg-zV-Tn"
   },
   "source": [
    "### A side note on documentation\n",
    "Woah! So many columns and abbreviations! What do they all mean? Make sure you know where your [dataset's documentation](https://wiki.humanconnectome.org/display/PublicData/HCP-YA+Data+Dictionary-+Updated+for+the+1200+Subject+Release#HCPYADataDictionaryUpdatedforthe1200SubjectRelease-Instrument:Demographics) is.\n",
    "\n",
    "Unfortunately, thorough documentation is not always available. Some data types are also very field-specific and require the help of experts, which is part of what makes data science so wonderfully interdisciplinary and why your own neuroscience backgrounds are extremely valuable!\n",
    "\n",
    "Let's take a look at what features we have here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLwc_rf0j19-"
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMyKyvPtj9cy"
   },
   "source": [
    "**Question 4:** The script above is not printing ALL of the columns... How do we fix that to be able to see all the features in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "3AWelNh_V-To"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data reformatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVZ3jtdSV-To"
   },
   "source": [
    "## 2.1 Selecting the features you want to work with:\n",
    "As you saw above, this dataframe has 499 features. Often, you will only want to work with a subset of the features of a dataframe, so you will have to create a new dataframe with this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBMC8C2QV-To"
   },
   "outputs": [],
   "source": [
    "basics = ['Subject','Gender','Age','PSQI_BedTime']\n",
    "df[basics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mve4NaXUV-To"
   },
   "source": [
    "Let's also add all of some cognitive variables to the mix! Specifically, we'll select the measures related to fluid intelligence (they start with `PMAT` for Penn Matrix Test) and impulsivity (they start with `DDisc` for Delay Discounting).\\\n",
    "\\\n",
    "**Question 5:** Fill in the missing code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSd4Fw-xV-To",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cognition = ['Subject','Gender','Age','PSQI_BedTime']\n",
    "for col in df.columns:\n",
    "    if (col.find(\"PMAT\")!=-1 or col.find(\"DDisc\")!=-1):\n",
    "        # FILL IN THE BLANK.\n",
    "print(f\"List of variables we will be looking at: {cognition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TyvOYGOV-To"
   },
   "source": [
    "**Question 6:** Now that we made a list of all of the features we want to examine, select this subset of our data and make it a separate dataframe called `df_cognition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "6cc_jyXLV-Tp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check to make sure everything is behaving as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Crko0pPjV-Tp"
   },
   "outputs": [],
   "source": [
    "df_cognition.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b12RXnhqV-Tp"
   },
   "source": [
    "## 2.2 Merging two dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_rBAqFSV-Tp"
   },
   "source": [
    "For our dataset of 1206 subjects, we have information about their gender, age range and a variety of cognitive measures. It would be interesting to integrate some other data as well. You have been provided with a separate file that contains brain structure volume data obtained from the neuroimaging data of these same subjects (note that not all subjects in the HCP have neuroimaging data) (more information about how these volumes were obtained can be found [here](https://github.com/NadiaBlostein/Open-Access-HCP-Data#readme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0WEmj8XV-Tp"
   },
   "outputs": [],
   "source": [
    "df_volumes = pd.read_csv(\"HCP_csv_data/HCP_volumes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tvCBCVslDLu"
   },
   "source": [
    "**Question 7:** How many subjects and features does `df_volumes` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IGrxEaawlCwp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAvipapJlXhm"
   },
   "source": [
    "**Question 8:** Print the mean and standard deviation of total brain volume (TBV) of this sample. Note that the unit is in mm$^{3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_MU4Tnw5V-Tp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9:** Can you round it to 4 decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M59oqzyHV-Tp"
   },
   "source": [
    "**Question 10:** List all the subjects whose TBV is above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Sv17Js45V-Tq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63BHKuYDV-Tq"
   },
   "source": [
    "Ok! Let's merge our dataframes. One problem is that our behavioral data has 1206 subjects and our volume data has 1086 subjects. \n",
    "\n",
    "**Question 11:** Create a new dataframe called `df_final` where we only keep the subjects for which we have all of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bF-AXKccV-Tq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check to make sure everything is behaving as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYLNBLnJmLcJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41UPVZxeV-Tq"
   },
   "source": [
    "Mission accomplished! We have 1086 and 35 features. However, we would normally expect 36 features (22 features from `df_cognition` and 14 features from `df_volumes`).\\\n",
    "\\\n",
    "**Question 12:** Why do we only have 35 features in `df_final`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Removing features that you do not need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UkGHuy8IgBd"
   },
   "source": [
    "**Question 13:** Complete the following code in order to find and remove any duplicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BlCUoH3wJk_V"
   },
   "outputs": [],
   "source": [
    "cols_to_drop=[]\n",
    "for i in range(df_final.shape[1]):\n",
    "    for j in range(i+1,df_final.shape[1]):\n",
    "        col1=df_final.columns[i]\n",
    "        col2=df_final.columns[j]\n",
    "        if (df_final[col1].equals(df_final[col2])):\n",
    "            print(f\"Duplicate columns: {col1, col2}\")\n",
    "            cols_to_drop.append(col2)\n",
    "# FILL IN THE BLANK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check to make sure everything is behaving as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykIY0AwO29no"
   },
   "outputs": [],
   "source": [
    "print(df_final.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk9oZn8rV-Tr"
   },
   "source": [
    "# 3.2 Making your data machine-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p9y9UQmV-Tr"
   },
   "source": [
    "To be machine-readable, your variables need to be numerical. Want to check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rD_Qg7vV-Tr"
   },
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFQsnbWlV-Tr"
   },
   "source": [
    "We have 3 columns that are non-numerical (meaning that they are neither floats nor integers): `Gender`,`Age`,`PSQI_BedTime`. Let's figure out how to handle them, one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDMZGn-7V-Tr"
   },
   "source": [
    "### One-hot encoding or binarizing your data\n",
    "First, we know that the `Gender` column is categorical and has two unique values: `M` or `F`. \n",
    "\n",
    "**Question 14:** Replace all of your `M` values with 1 and `F` values with 2. Hint: `.replace()` can be quite helpful. \\\n",
    "\\\n",
    "_PS: Always make sure to **save your data preprocessing code**. If ever you forget what number you assigned to what value in the `Gender` category, you will always be able to look back at your code!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Oa77oNphV-Tr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q22Um_pUV-Tr"
   },
   "source": [
    "However, suppose that you actually had more than 2 numerical values for this feature (e.g. `M`,`F`,`other`). If you just convert categorical variables to numerical values (ex: `M`=1,`F`=2,`other`=3), you give a \"distance\" to the relationship between variables. For instance, since 1 is closer to 2 than to 3, you are telling your machine that `M` is \"closer\" to `F` (`distance = 2 - 1 = 1`) than to `other` (`distance = 3 - 1 = 2`). We want our categories to be independent. That's where one hot encoding comes into play: \"[A representation of categorical variables as binary vectors](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/).\"\n",
    "\n",
    "Let's revert our `Gender` feature back to its original form, and see how to one-hot encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Gender'] = df_final['Gender'].replace(1,'M')\n",
    "df_final['Gender'] = df_final['Gender'].replace(2,'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df_final['Gender'])\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we convert 1 categorical feature into N binary features, where N = number of values that your feature can take on (2 in this example).\\\n",
    "\\\n",
    "**Question 15:** Add `one_hot` to `df_final` and make sure to drop the `Gender` column (to avoid duplicate information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you need to reassign your changes to `df_final` (`df_final = `), or they will not get saved. Quick sanity check to make sure everything is behaving as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xuti0j5VV-Ts"
   },
   "source": [
    "### Parsing strings in your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_Fexj30V-Ts"
   },
   "source": [
    "#### Handling the `Age` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGTxCDCvV-Ts"
   },
   "source": [
    "Remember that our `Age` feature is also non-numerical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMaLuwlPV-Ts"
   },
   "outputs": [],
   "source": [
    "df_final['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6izSxt7eV-Ts"
   },
   "source": [
    "**Question 16:** For the sake of this exercise, replace 36+ with a range of 36-40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GlY65tn4a0h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check to make sure everything is behaving as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSWS24gl4bOz"
   },
   "source": [
    "Our age range values are organized very clearly: `minimum age â€“ maximum age`. These age range strings can therefore be split around the `-` such that you create two new columns: one for `minimum age` and one for `maximum age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWGKnhdcV-Ts"
   },
   "outputs": [],
   "source": [
    "fix_age = df_final['Age'].str.split('-', 1, expand=True)\n",
    "fix_age.columns = ['min','max']\n",
    "fix_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpPpU5DQnfq9"
   },
   "source": [
    "**Question 17:** In `df_final`, replace the age range string (from the `Age` column) for each subject with the mean of the subject's respective age range. _Hint: make sure that the `min` and `max` columns of the `fix_age` df are converted to a numerical data type._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Uwor8DLqV-Ts"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyE6rGXV4tm6"
   },
   "outputs": [],
   "source": [
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks8toV84V-Ts"
   },
   "source": [
    "#### Handling the `PSQI_BedTime` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs_CxE3HV-Ts"
   },
   "source": [
    "Convert your bed time variable from HH:MM:SS to seconds! The next line of code uses some slightly more advanced functions that will not be covered today, but you can walk through it step by step to figure out what each method does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bnziq-1QV-Tt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ftr = [3600,60,1]\n",
    "for i in range(len(df_final['PSQI_BedTime'])):\n",
    "    x = sum([a*b for a,b in zip(ftr, map(int,df_final['PSQI_BedTime'][i].split(':')))])\n",
    "    df_final['PSQI_BedTime'][i] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjuGPr07V-Tt"
   },
   "source": [
    "#### A note on other strings that often crop up in dataframes and need to be replaced with numbers!\n",
    "`df_final = df_final.replace('FALSE',0)` \\\n",
    "`df_final = df_final.replace('TRUE',1)` \\\n",
    "`df_final = df_final.replace(False,0)` \\\n",
    "`df_final = df_final.replace(True,1)` \\\n",
    "`df_final = df_final.replace('0',0)` \\\n",
    "`df_final = df_final.replace(' ',np.NaN)` # replacing random spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQenax9dV-Tt"
   },
   "source": [
    "**Question 18:** Write a quick line of code that makes sure that every column of `df_final` is of type float!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qu2IPUdZV-Tt",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP_h6HlKV-Tt"
   },
   "source": [
    "## 3.3 Handling not available (NA) and inf data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfiQZysiV-Tt"
   },
   "source": [
    "Sometimes, Python will convert some of your values to + or - infinity, which will result in downstream errors. Convert them to NA, and then handle them as NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXTXFDNVV-Tt"
   },
   "outputs": [],
   "source": [
    "df_final = df_final.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QX2xgJzkV-Tt"
   },
   "source": [
    "Next, you need to deal with your NA values. \n",
    "\n",
    "**Question 19:** How many nas do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RF_6bB7wV-Tt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKCftVkdV-Tt"
   },
   "source": [
    "There is a variety of ways to handle NA data. The most simple approach is to replace NA data with the median (or mean) value of the feature of interest. There are [other](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779) [more](https://arxiv.org/abs/1804.11087) sophisticated data imputation techniques out there, many of which actually leverage machine learning tools.\n",
    "\n",
    "However, if a feature has too many NAs, you may want to remove it completely. Define a threshold for the minimal number of missing values that qualifies a feature for removal from your dataset. Here, 12 is quite stringent.\n",
    "\n",
    "**Question 20:** Fill in the blank line of the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=12\n",
    "\n",
    "remove_cols = []\n",
    "for i in range(len(df_final.columns)):\n",
    "    if (df_final.iloc[:,i].isnull().sum() >= threshold):\n",
    "        remove_cols.append(df_final.columns[i])\n",
    "# FILL IN THE BLANK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 21:** Replace the NA values of this dataframe with the feature-specific median (the median is more robust against outliers than the mean is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Lad9NHw9V-Tt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check to make sure everything is behaving as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9RH8R3LV-Tu"
   },
   "outputs": [],
   "source": [
    "df_final.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference between df_final.isna().sum() and df_final.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Removing columns with a standard deviation of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 22:** Write a line of code that removes all the features (ie columns) with a standard deviation of 0. Why do you think we would want to do this?\\\n",
    "\\\n",
    "_**Caution:** Depending on the research question you are asking, you may not want to remove features that have a standard deviation of zero. Indeed, although a feature that has the same value across all subjects within a sample does not give you useful information about sample variance, you are still learning something about your sample itself._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check to make sure everything is behaving as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_final.columns:\n",
    "    if (df_final[col].std()==0): print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You usually need to perform some sort of feature scaling / normalization to make sure that all of your variables are in the same range (this affects gradient-descent-based algorithms and distance-based algorithms).\n",
    "\n",
    "**Min-Max Scaling / Normalization:** X' = (X-Xmin) / (Xmax-Xmin) $\\rightarrow$ _X' ends up ranging from 0 to 1_ \\\n",
    "**Standardization / Standard Scaler / Z-score:** X' = (X-$\\mu$)/$\\sigma$\n",
    "\n",
    "Which to use? Depends on your data! \n",
    "* \"Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution\" (read more on this [here](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)). \n",
    "* Other [popular scaling techniques](https://www.analyticsvidhya.com/blog/2020/07/types-of-feature-transformation-and-scaling/) include the log transform (you often see this with GWAS, ie genome-wide association studies) and dividing your column-wise values by the absolute value of the maximal value of each column (max abs scaler)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaxScaled_df_final=(df_final-df_final.min())/(df_final.max()-df_final.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Standard Scaling\\\n",
    "\\\n",
    "**Question 23:** Implement a line of code that performs the following column-wise z-scoring (X' = (X-$\\mu$)/$\\sigma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3: Sklearn Min-Max Scaler\\\n",
    "Slightly different from the Min-Max Scaling defined above:\\\n",
    "`\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms=MinMaxScaler()\n",
    "mms.fit(df_final)\n",
    "df_final_mms=mms.transform(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qZ-YOcWV-Tu"
   },
   "source": [
    "# 5. Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzQT3zchV-Tu"
   },
   "source": [
    "Data visualization is a wonderful way to get to know your data in order to plan a relevant analysis or find an appropriate machine learning application. [Matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/) are two canonical data visualization tools that you can use in Python. You will be learning more about it on Wednesday (module content can be found [here](https://github.com/neurodatascience/course-materials-2022/tree/main/Lectures/09-Intro_to_Data_Visualization)). Below is but a taster for what lies ahead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_L_6wczKV-Tu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fekJR587V-Tv"
   },
   "source": [
    "#### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOwRnWU2V-Tv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df_final[\"PSQI_BedTime\"],df_final[\"TBV\"]) # plot (x, y)\n",
    "plt.ylabel(\"Total brain volume (mm3)\") # label the y-axis\n",
    "plt.xlabel(\"Bedtime (seconds)\") # label the x-axis\n",
    "plt.title(\"Total brain volume as a function of bed time\") # title your graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGmR_x-Aq-QB"
   },
   "source": [
    "**Question 24:** Make a scatter plot of total brain volume (TBV) as a function of bed time, by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhLbR9ZlV-Tw"
   },
   "source": [
    "#### Histogram plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwQf03YAV-Tx"
   },
   "outputs": [],
   "source": [
    "sns.displot(df_final,x='TBV',kind='kde',fill=True) # plot your histogram\n",
    "plt.ylabel(\"Density\") # name y-axis\n",
    "plt.xlabel(\"TBV (mm3)\") # name x-axis\n",
    "plt.title(\"Distribution of total brain volume (TBV) in our sample\") # figure title\n",
    "plt.gcf().set_size_inches(8, 5) # another way to adjust figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTA7DWybV-Tx"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df_final,x='Str_Left',fill=True,color='orange')\n",
    "    # first, plot the distribution of the left striatum in orange\n",
    "sns.histplot(df_final,x='Thal_Left',fill=True,color='turquoise')\n",
    "    # second, plot the distribution of the left thalamus volume in turquoise\n",
    "plt.legend(['Left striatum','Left thalamus'])\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Structure-specific volume (mm3)\")\n",
    "plt.title(\"Distribution of different structure volumes in our sample\")\n",
    "plt.gcf().set_size_inches(8, 5) # adjust figure size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvaFPk-5rUrw"
   },
   "source": [
    "**Question 25:** Generate two smoothed and superimposed histograms of bed times in subjects that are above and below 30 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ju817ns-rV7V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7kKZDlFsTlV"
   },
   "source": [
    "Notice that brains seem to shrink with age in our dataset... Let's look at the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iviSqsSXtFMv"
   },
   "outputs": [],
   "source": [
    "TBV_below_25=df_final['TBV'][df_final[\"Age\"]<=25].mean()\n",
    "TBV_above_25=df_final['TBV'][df_final[\"Age\"]>25].mean()\n",
    "print(f\"Average TBV for people below or equal to 25 years of age: {round(TBV_below_25,0)} mm3\") \n",
    "print(f\"Average TBV for people above 25 years of age: {round(TBV_above_25,0)} mm3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7mhDCIFtdBi"
   },
   "source": [
    "Interesting! However, does our sample actually have a similar amout of people who are below and above the age of 25? \n",
    "\n",
    "**Question 26:** Count how many people are <= 25 years, and how many people are > 25 yrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KF6kiQdPtrdt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Handling 2D Images in Python\n",
    "There are many different Python tools that you can use to convert 2D images into numpy arrays that you can then manipulate in various ways. For more information, check out [10 Python image manipulation tools](https://opensource.com/article/19/3/python-image-manipulation-tools). In the following (brief) tutorial, we will be using **s**ci**k**it image (**sk**image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd HCP_2D_slices_MRI_data\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an image into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "filename = 'HCP_102109_T1w_acpc_dc_restore_brain_t1_axial.png'\n",
    "img=io.imread(filename)\n",
    "print(f\"Shape: {img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 27:** Why does is it a tensor with 4 \"channels\"? _Hint: this in a .png, not a .jpg, image!\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a script that displays all of your axial images:\\\n",
    "_PS: The `glob` Python module iterates through the directory of interest by interacting directly with your command line._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "images=[]\n",
    "for file in glob.iglob('HCP*axial*png'):\n",
    "    img=io.imread(file) \n",
    "    images.append(img)\n",
    "fig = plt.figure(figsize=(15, 3.5))\n",
    "rows = 1\n",
    "columns = 5\n",
    "for i in range(5):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    img=images[i]\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Shape: {img.shape}\",size=12) # height x width [x channel]\n",
    "plt.suptitle(\"AXIAL VIEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 28:** Write a for loop to load & display all of your **sagittal** and **coronal** views of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 29:** To relate our pandas dataframe (`df_final`) to our 2D image data, display the 2D images (3 views) of `subject 995174` and find out the age and gender of this subject. **Making your code more generalizable:** start with `Subject_ID = 995174` such that the subject ID becomes a variable. That way, you only need to replace the value that you assign `Subject_ID` variable in order to examine the same information in another subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image processing and scipy: the [ndimage](https://docs.scipy.org/doc/scipy/reference/ndimage.html#module-scipy.ndimage) package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the first image we looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'HCP_102109_T1w_acpc_dc_restore_brain_t1_axial.png'\n",
    "img=io.imread(filename)\n",
    "print(f\"Shape: {img.shape}\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the documentation and explore the available filters! Let's start by rotating our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgRotated = ndimage.rotate(img, 45, reshape=False)\n",
    "plt.imshow(imgRotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 30:** Is there a way to rotate this image such that you are not cropping it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image smoothing (ie blurring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's blur our image using a Gaussian filter with varying $\\sigma$ values!\\\n",
    "**A note on image smoothing:** The image smoothing technique that uses a Gaussian **filter** (ie **kernel**) is just one of many possible image smoothing techniques. Smoothing (ie \"**blurring**\") an image is often necessary in order to reduce its noise or to reduce the image resolution. Lowering image resolution can be valuable to make certain pipelines less computationally demanding. Why do you think this would be? Although the purpose of this module is NOT to teach you about image smoothing, you can read more about it [here](https://matthew-brett.github.io/teaching/smoothing_intro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgBlur = ndimage.gaussian_filter(img, sigma=2)\n",
    "plt.imshow(imgBlur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image looks strange because our Gaussian filter is also blurring the \"transparency\" value (recall that we are working with a 4-channel `png` image). Let's convert our image to Grayscale (ie 1 channel) to make our blurring more interpretable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "imgGray = color.rgb2gray(img)\n",
    "print(f\"Shape: {imgGray.shape}\")\n",
    "plt.imshow(imgGray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our converted image still looks identical to the original one with 4 channels. Amazing! Let's see what our Gaussian filter does now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgBlur = ndimage.gaussian_filter(imgGray, sigma=2)\n",
    "plt.imshow(imgBlur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 31:** Fill in the blank lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_vals=[0.1,0.5, 1, 2, 3, 4,5,6]\n",
    "\n",
    "images=[]\n",
    "# FILL IN THE BLANK.\n",
    "    img=ndimage.gaussian_filter(imgGray, sigma=sig)\n",
    "    images.append(img)\n",
    "    \n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "rows = 1\n",
    "columns = len(sigma_vals)\n",
    "for i in range(columns):\n",
    "    fig.add_subplot(1, columns, i+1)\n",
    "    img=images[i]\n",
    "# FILL IN THE BLANK.\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Kernel size: {sigma_vals[i]}\",size=12)\n",
    "plt.suptitle(\"GAUSSIAN BLURRING\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week0-3_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
