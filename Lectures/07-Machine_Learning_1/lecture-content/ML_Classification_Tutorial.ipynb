{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-Learning tutorial for classification\n",
    "\n",
    "## Objective\n",
    "1. Predict diagnostic label from functional connectome\n",
    "2. Predict MRI acquisition site from functional connectome\n",
    "\n",
    "## Dataset\n",
    "[ABIDE (Autism)](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_abide_pcp.html)\n",
    "\n",
    "## Preprocessing\n",
    "1. Generate region-to-region connectivity matrix (i.e. connectome) from functional timeseries data\n",
    "    - ROIs are defined based on [harvard_oxford atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_harvard_oxford.html) or [AAL atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_aal.html)\n",
    "2. Apply dimensionality reduction (Optional)\n",
    "\n",
    "## Model \n",
    "1. Logistic regression\n",
    "2. Random Forest\n",
    "\n",
    "## Cross-validation\n",
    "1. k-fold\n",
    "2. shuffle-split\n",
    "\n",
    "## post-hoc analysis\n",
    "Compare model performance for predicting Dx labels vs. MRI acquisition site\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's begin! \n",
    "### First we import some useful python libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from nilearn import datasets\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "### ROIs are defined based on [harvard_oxford atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_harvard_oxford.html) or [AAL atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_aal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_subjects = 1000\n",
    "parcel = 'rois_ho' # 'rois_ho' or 'rois_aal\n",
    "data = datasets.fetch_abide_pcp(n_subjects=n_subjects,derivatives=[parcel],data_dir='./') #17:28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotypes and Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno = pd.DataFrame(data['phenotypic']).drop(columns=['i','Unnamed_0'])\n",
    "pheno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_counts = pheno['SITE_ID'].value_counts()\n",
    "dx_counts = pheno['DX_GROUP'].value_counts()\n",
    "\n",
    "print(f'Dx count:\\n{dx_counts}\\n\\nScanning site_counts:\\n{site_counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRI features\n",
    "\n",
    "### These are stored in a list, where each list element is a subject-specific feature matrix\n",
    "### Subject specific feature matrix: timepoints x ROIs\n",
    "### ROIs are defined based on [harvard_oxford atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_harvard_oxford.html) or [AAL atlas](https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_atlas_aal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[parcel]\n",
    "\n",
    "print(f'Number of samples: {len(features)}')\n",
    "\n",
    "subject_feature_shape = features[0].shape\n",
    "\n",
    "print(f'subject_feature_shape: {subject_feature_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the atlas looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "if parcel == 'rois_ho':\n",
    "    atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "else:\n",
    "    atlas = datasets.fetch_atlas_aal()\n",
    "\n",
    "plotting.plot_roi(atlas.maps, draw_cross=False, title=parcel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the subject-specific feature matrix\n",
    "### Here the feature set is organized in a (time x roi) matrix per individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15,10))\n",
    "g = sns.heatmap(features[0].T, ax=ax)\n",
    "g.set_xlabel('timepoint')\n",
    "g.set_ylabel('ROI')\n",
    "g.set_title('Functional data timeseries')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing / feature engineering\n",
    "\n",
    "### Commonly functional (timeseries) neuroimaging data is represented as functional connectome aka network aka graph. \n",
    "### Here we convert the (time x roi) matrix into (roi x roi) matrix per individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_matrix = ConnectivityMeasure(kind='correlation')\n",
    "connectome_matrix = connectome_matrix.fit_transform([features[0]])[0]\n",
    "print(f'Shape connectome: {connectome_matrix.shape}')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15,10))\n",
    "g = sns.heatmap(connectome_matrix, ax=ax)\n",
    "g.set_xlabel('ROI')\n",
    "g.set_ylabel('ROI')\n",
    "g.set_title('Connectome')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract lower (or upper) triangle entrees (excluding diagonal)\n",
    "### Here we convert the (roi x roi) matrix into a single feature vector per individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril_idx = np.tril_indices(len(connectome_matrix),k=1) \n",
    "features_flat = connectome_matrix[tril_idx]\n",
    "print(f'Number of features per subject: {len(features_flat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do this for all subjects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defs are definitely useful! \n",
    "\n",
    "def extract_connectome_features(func_data, measure):\n",
    "    ''' A function to calculate connnectome based on timeseries data and similarity measure\n",
    "    '''\n",
    "    # Build connectom matrix\n",
    "    # connectome_matrix =\n",
    "    \n",
    "    # Extract lower triangular indices\n",
    "    # tril_idx = \n",
    "    \n",
    "    # Grab the lower trianglar features\n",
    "    # flat_features = \n",
    "\n",
    "    return flat_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: here we are pre-processing each image independently i.e. not using any group-level information for scaling / normalization / feature transformation (e.g. PCA). Therefore there is no \"double dipping\" or leakage of information from a test set. This sort of independent image pre-processing, we can do on entire dataset without creating train-test splits first and then defining feature-set on the training data only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "flat_features_list = []\n",
    "for func_data in features:\n",
    "    flat_features = extract_connectome_features(func_data, correlation_measure)\n",
    "    flat_features_list.append(flat_features)\n",
    "\n",
    "print(f'Length of flat_features_list {len(flat_features_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data matrix (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(flat_features_list)\n",
    "\n",
    "print(f'Input data (X) shape: {X.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output labels (y): Diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "outcome = 'DX_GROUP' \n",
    "y = pheno[outcome]\n",
    "y_counts = y.value_counts()\n",
    "\n",
    "print(f'Unique output clasess:\\n{y_counts}')\n",
    "\n",
    "# Encode labels to integer categories\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay now we have our input data (X) and output data (y) in the following format\n",
    "<img src=\"./QLS_ML_terminology.png\" alt=\"terms\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-test split\n",
    "- 80/20 ratio\n",
    "- Stratify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_subset_fraction = 0.2 # \n",
    "stratification = y \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                    X, # input features\n",
    "                                                    y, # output labels\n",
    "                                                    test_size = test_subset_fraction, \n",
    "                                                    shuffle = True, # shuffle dataset\n",
    "                                                                    # before splitting\n",
    "                                                    stratify = stratification, \n",
    "                                                    random_state = 123 # same shuffle each time\n",
    "                                                    )\n",
    "\n",
    "# print the size of our training and test groups\n",
    "print('training:', len(X_train), 'testing:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay finally, let's train your first model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = 'LR' # 'LR' or 'RF'\n",
    "\n",
    "if model == 'RF':\n",
    "    clf = RandomForestClassifier(max_depth=3, class_weight='balanced', random_state=0)\n",
    "elif model == 'LR':\n",
    "    clf = LogisticRegression(penalty='l1', C=1, class_weight='balanced', random_state=0)\n",
    "else:\n",
    "    print(f'Unknown model: {model}')\n",
    "\n",
    "print(f'Using model: {model}')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "print(f'train acc: {train_acc:.3f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set\n",
    "- accuracy\n",
    "- confusion_matrix\n",
    "- precision_recall_fscore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(f'test acc: {test_acc:.3f}')\n",
    "\n",
    "test_cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Are we overfitting? \n",
    "\n",
    "## Q2: What can we do to mitigate overfitting? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,5))\n",
    "g = sns.heatmap(test_cm, annot=True, ax=ax)\n",
    "g.set_title('Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,r,f1,_ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'model: {model}, outcome: {outcome}\\n Acc:{test_acc:.2f}, precision: {p:.2f}, recall: {r:.2f}, f1: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try to predict scan site from the same features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'SITE_ID'\n",
    "y = pheno[outcome]\n",
    "y_counts = y.value_counts()\n",
    "\n",
    "print(f'Unique output clasess:\\n{y_counts}')\n",
    "\n",
    "# Encode labels to integer categories\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e0ccb70fefdc43ea196e5bed1dad258852fcc07a4658b68f0531d3356971284"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
